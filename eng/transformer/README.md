# Transformers

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TyMill/ml-starter-kit/blob/main/eng/transformer/transformer_tutorial.ipynb)
[![Try on Kaggle](https://img.shields.io/badge/Open%20in-Kaggle-blue)](https://www.kaggle.com/code)
[![JupyterLite](https://img.shields.io/badge/Try%20it-JupyterLite-orange)](https://jupyterlite.github.io/demo)

**Transformers** are cutting-edge deep learning models based on self-attention mechanisms.  
They have revolutionized natural language processing (NLP) and are the backbone of models like BERT, GPT, T5, and more.

---

## ğŸ” What is a Transformer?

- Uses **self-attention** instead of recurrence to model sequence relationships  
- Can **process input in parallel**, making training faster  
- Learns **contextual representations** of words and sentences  
- Foundation for modern NLP and vision models

---

## âœ… Advantages

- Highly parallelizable  
- Effective for long-range dependencies  
- Works well for both text and vision tasks  
- Enables pretraining on massive datasets

## âŒ Disadvantages

- Requires large amounts of data and compute  
- Can be harder to interpret  
- Pretrained models may have biases

---

## ğŸ§ª Whatâ€™s Inside the Tutorial

- ğŸ“š Load a text dataset (`AG News`)  
- ğŸ§  Tokenize using `BertTokenizer`  
- ğŸ” Extract contextual embeddings using `BertModel`  
- ğŸ“Š Visualize the `CLS` token embedding from the transformer output

ğŸ“˜ Notebook: [`transformer_tutorial.ipynb`](./transformer_tutorial.ipynb)

---

## ğŸ“‚ File Structure

- `transformer_tutorial.ipynb` â€“ HuggingFace-based tutorial  
- `README.md` â€“ This file  
- `references.md` â€“ Learning resources and official docs

---

## ğŸ“š Recommended Reading

- [Hugging Face Transformers Docs](https://huggingface.co/docs/transformers/index)  
- [Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)  
- [Wikipedia: Transformer (Machine Learning)](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))

---

## ğŸš€ Try It Yourself

- Replace `BertModel` with `DistilBERT`, `RoBERTa`, or `GPT2`  
- Use outputs for classification, clustering, or sentiment analysis  
- Tokenize longer documents with truncation or sliding window  
- Use `Trainer` from HuggingFace to fine-tune models

---

Created with ğŸ§  by **Tymoteusz Miller** â€” part of the [ML Starter Kit](https://github.com/TyMill/ml-starter-kit)
