# Cheatsheet: Machine Learning Algorithms

| Algorithm              | Type              | Best Use Case                    | Key Formula / Logic                                 | Assumptions               | Pros                              | Cons                                | When NOT to Use                                   | Real-World Example                  |
|------------------------|-------------------|----------------------------------|-----------------------------------------------------|---------------------------|------------------------------------|-------------------------------------|----------------------------------------------------|-------------------------------------|
| Linear Regression      | Supervised        | Predicting continuous values     | Y = b0 + b1X + ...                                  | Linearity, independence   | Simple, interpretable, fast        | Sensitive to outliers               | Non-linear data, strong variable correlation       | House price prediction              |
| Logistic Regression    | Supervised        | Binary classification            | P = 1 / (1 + e^-(b0 + b1X + ...))                   | Log-odds linearity         | Probabilistic, interpretable       | Weak with non-linear data           | Strong non-linearity in data                        | Spam detection                      |
| Decision Tree          | Supervised        | Classification / Regression      | Recursive binary splitting                          | None                      | Easy to interpret, no assumptions | Overfitting, unstable               | Complex dependencies, low data                      | Loan default prediction             |
| Random Forest          | Supervised        | Accuracy improvement              | Bagging + averaging                                | Tree independence          | High accuracy, robust             | Slower, less interpretable          | When model explainability is needed                 | Fraud detection                     |
| Gradient Boosting      | Supervised        | High-performance modeling        | Minimize loss through boosting                      | Sequential dependency      | State-of-the-art accuracy         | Overfitting, slower                 | Interpretability is crucial                         | Credit scoring                      |
| SVM                    | Supervised        | Max-margin classification        | Maximize margin with kernel trick                  | Separability, scaling      | Works well in high dimensions     | Slow on large datasets              | Large datasets                                      | Facial recognition                  |
| KNN                    | Supervised        | Few-shot classification          | Distance-based majority voting                      | Feature scaling            | Simple, no training phase         | Slow, noisy sensitivity             | High-dimensional or noisy data                     | Recommender systems                |
| Naive Bayes            | Supervised        | Text classification              | Bayes theorem + feature independence                | Feature independence       | Fast, good for text               | Fails with correlated features      | Strong feature dependency                            | Sentiment analysis                  |
| K-Means                | Unsupervised      | Customer segmentation            | Minimize intra-cluster distance                     | Spherical clusters         | Fast, easy to implement           | Needs K, sensitive to shape         | Non-spherical, varying size clusters                | Customer segmentation               |
| Hierarchical Clust.    | Unsupervised      | Data structure understanding     | Nested dendrogram                                   | Distance metric            | No need to specify K              | Memory & compute intensive          | Large datasets                                       | Gene expression analysis            |
| PCA                    | Dim. Reduction    | Feature dimensionality reduction | Eigenvectors of covariance matrix                   | High variance = important  | Noise reduction, speed-up         | Hard to interpret results           | When all features are important                    | Image compression                   |
| Neural Networks (MLP)  | Supervised        | Complex pattern recognition      | Weighted sum + activation functions                 | Enough data, scaling       | Non-linear learning power         | Needs large data & tuning           | Low data, explainability matters                   | Image classification                |
| CNN                    | Supervised        | Image/video/audio classification | Convolution + pooling layers                        | Grid-structured input       | Excellent for images              | High compute resources              | Sequential/text data                                  | Self-driving cars                   |
| RNN                    | Supervised        | Sequence modeling                | Feedback loops over time                            | Sequential data            | Time-aware, memory                | Vanishing gradient problem          | Long sequences                                      | Stock prediction                    |
| Transformer (BERT, GPT)| Supervised/Self   | NLP, chat, translation           | Attention + position encoding                       | Large dataset              | Long context, fast                | Heavy compute, large models         | Small projects                                     | ChatGPT, Translation tools          |
| Autoencoders           | Unsupervised      | Compression & anomaly detection  | Encoder-decoder + reconstruction loss               | Symmetric networks         | Effective denoising               | Can overfit, black-box              | No compression needed                               | Fraud detection                     |
| DBSCAN                 | Unsupervised      | Arbitrary shape clustering       | Density-based region growing                        | Cluster density            | Noise tolerant, shape-flexible    | Fails on varying density            | Sparse high-dimensional data                        | Geo-spatial clustering              |
| GMM (Gaussian Mixture)  | Unsupervised     | Probabilistic clustering        | Mixture of multiple Gaussian distributions        | Approx. Gaussian data      | Models overlapping clusters      | Sensitive to initialization         | Data not normally distributed                        | Image segmentation, bioinformatics  |

---

Created by **Tymoteusz Miller**  
This repository contains tutorials for each algorithm â€“ check the folders for step-by-step guides!
